// FILE: supabase/functions/dipsy-training-evaluator/index.ts
// Purpose:
// - Offline training-loop evaluator for Dipsy's Atlas Questions Brain.
// - Reads recent interactions from dipsy_interaction_log where agent_type = 'questions_brain'.
// - Sends each (question, answer) pair to an Evaluator agent (OpenAI).
// - Stores the structured evaluation JSON into dipsy_training_evaluations.
// - ALSO syncs evaluation into dipsy_training_examples (same interaction) so the
//   rewriter + UI can see verdict/score/rewrite_recommended.
//
// Security:
// - NOT called from the browser.
// - Only via secure backend jobs (cron, CLI, postgres cron, etc.).
// - Requires ?token=... in the URL that matches DIPSY_TRAINING_TOKEN env.
// - Uses SUPABASE_SERVICE_ROLE_KEY (no RLS weakening for clients because this
//   function itself is never exposed to them).

import { serve } from "https://deno.land/std@0.224.0/http/server.ts";
import { createClient } from "https://esm.sh/@supabase/supabase-js@2.49.1";

const SUPABASE_URL = Deno.env.get("SUPABASE_URL") ?? "";
const SUPABASE_SERVICE_ROLE_KEY = Deno.env.get("SUPABASE_SERVICE_ROLE_KEY") ?? "";
const OPENAI_API_KEY = Deno.env.get("OPENAI_API_KEY") ?? "";

// CORS (mostly for debugging from curl/postman)
const corsHeaders: Record<string, string> = {
  "Access-Control-Allow-Origin": "*",
  "Access-Control-Allow-Headers": "authorization, x-client-info, apikey, content-type",
  "Access-Control-Allow-Methods": "POST, OPTIONS",
};

// ---------------- Types ----------------

type InteractionRow = {
  id: string;
  org_id: string | null;
  user_id: string | null;
  channel: string | null;
  agent_type: string | null;
  question: string | null;
  answer: string | null;
  created_at: string | null;
};

type EvaluationJson = {
  overall_score: number;
  dimensions: {
    accuracy: number;
    grounding: number;
    clarity: number;
    completeness: number;
    style_tone: number;
  };
  verdict:
    | "excellent"
    | "good"
    | "good_but_improvable"
    | "needs_revision"
    | "unsafe_or_incorrect";
  issues: {
    dimension: "accuracy" | "grounding" | "clarity" | "completeness" | "style_tone";
    severity: "low" | "medium" | "high";
    description: string;
  }[];
  rewrite_recommended: boolean;
  rewrite_priority: "low" | "medium" | "high";
  notes_for_rewriter: string;
};

// ---------------- System Prompt ----------------

const EVALUATOR_SYSTEM_PROMPT = `
You are the Dipsy Questions Brain Evaluator for Atlas Command.

Your single job:
- Evaluate a single Q&A pair coming from the Atlas Questions Brain (product FAQ brain).
- Judge whether the ANSWER is accurate, grounded, clear, complete, and stylistically aligned with Atlas.
- Output ONLY a single JSON object with your evaluation. No extra commentary.

Context:
- The "question" is a user's question about how Atlas works as a product (UI, statuses, workflows, billing, documents, etc.).
- The "answer" was generated by the Atlas Questions Brain using atlas_docs as its only knowledge source.
- The answer SHOULD be:
  - Correct and consistent with Atlas docs.
  - Explicit when something is unknown or not implemented yet.
  - Clear and structured (short paragraphs, bullets).
  - Helpful but not fluffy.
  - Neutral, professional, a bit friendly but not silly.

VERY IMPORTANT:
- If you are unsure about the factual correctness of the answer, err on the side of LOWER accuracy/grounding scores.
- If the answer looks like it's guessing or speculating, that is a serious grounding problem.
- If the answer is correct but wordy, that affects "clarity" and "style_tone", not "accuracy" or "grounding".

Input (you will ALWAYS be given this as JSON in the user message):

{
  "question": "<user question string>",
  "answer": "<questions brain answer string>"
}

Your output MUST be a single JSON object with this exact shape:

{
  "overall_score": number,  // 0.0 to 1.0
  "dimensions": {
    "accuracy": number,     // 0.0 to 1.0
    "grounding": number,    // 0.0 to 1.0 (no guessing, no hallucinations)
    "clarity": number,      // 0.0 to 1.0 (easy to understand, structured)
    "completeness": number, // 0.0 to 1.0 (did it address the whole question?)
    "style_tone": number    // 0.0 to 1.0 (matches Atlas voice: concise, helpful, professional)
  },
  "verdict": "excellent" | "good" | "good_but_improvable" | "needs_revision" | "unsafe_or_incorrect",
  "issues": [
    {
      "dimension": "accuracy" | "grounding" | "clarity" | "completeness" | "style_tone",
      "severity": "low" | "medium" | "high",
      "description": "Short natural language description of the issue."
    }
  ],
  "rewrite_recommended": boolean,
  "rewrite_priority": "low" | "medium" | "high",
  "notes_for_rewriter": "Short guidance for a rewriter agent. Be specific about what to change and what to KEEP."
}

Scoring guidance:
- "excellent": overall_score >= 0.9 and no significant issues.
- "good": overall_score between 0.8 and 0.9, only minor issues.
- "good_but_improvable": overall_score between 0.6 and 0.8, some issues but answer is safe to use.
- "needs_revision": overall_score between 0.3 and 0.6, important issues (clarity or completeness).
- "unsafe_or_incorrect": overall_score < 0.3, serious accuracy/grounding problems.

rewrite_recommended:
- true if verdict is "good_but_improvable", "needs_revision", or "unsafe_or_incorrect".
- false only if the answer is already "excellent" or solid "good".

IMPORTANT:
- Do NOT propose an actual rewrite here.
- Do NOT include anything outside the JSON object.
- Do NOT change keys or add extra top-level fields.
`.trim();

// ---------------- HTTP Handler ----------------

serve(async (req: Request): Promise<Response> => {
  try {
    if (req.method === "OPTIONS") {
      return new Response("ok", { headers: corsHeaders });
    }

    if (req.method !== "POST") {
      return jsonResponse(405, { ok: false, error: "Method not allowed" });
    }

    const url = new URL(req.url);
    const token = url.searchParams.get("token") ?? "";

    const DIPSY_TRAINING_TOKEN = (Deno.env.get("DIPSY_TRAINING_TOKEN") ?? "").trim();

    if (!DIPSY_TRAINING_TOKEN) {
      console.error("[dipsy-training-evaluator] Missing DIPSY_TRAINING_TOKEN env");
      return jsonResponse(500, {
        ok: false,
        error: "Training evaluator misconfigured (missing token env).",
      });
    }

    if (!token || token !== DIPSY_TRAINING_TOKEN) {
      return jsonResponse(401, {
        ok: false,
        error: "Unauthorized: invalid training token.",
      });
    }

    if (!SUPABASE_URL || !SUPABASE_SERVICE_ROLE_KEY) {
      console.error("[dipsy-training-evaluator] Missing SUPABASE_URL or SERVICE_ROLE key");
      return jsonResponse(500, {
        ok: false,
        error: "Training evaluator misconfigured (missing Supabase env).",
      });
    }

    if (!OPENAI_API_KEY) {
      console.error("[dipsy-training-evaluator] Missing OPENAI_API_KEY");
      return jsonResponse(500, {
        ok: false,
        error: "Training evaluator misconfigured (missing OpenAI key).",
      });
    }

    const body = await req.json().catch(() => ({} as any));
    const limitRaw = typeof body.limit === "number" ? body.limit : 20;
    const limit = Math.max(1, Math.min(limitRaw, 50));

    const supabase = createClient(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY, {
      auth: { persistSession: false },
    });

    console.log(`[dipsy-training-evaluator] Starting evaluation run. limit=${limit}`);

    // 1) Fetch recent questions_brain interactions
    const { data: interactions, error: logErr } = await supabase
      .from("dipsy_interaction_log")
      .select(
        `
        id,
        org_id,
        user_id,
        channel,
        agent_type,
        question,
        answer,
        created_at
      `,
      )
      .eq("agent_type", "questions_brain")
      .order("created_at", { ascending: false })
      .limit(limit);

    if (logErr) {
      console.error("[dipsy-training-evaluator] Log fetch error:", logErr);
      return jsonResponse(500, {
        ok: false,
        error: logErr.message || "Failed to fetch interaction logs.",
      });
    }

    if (!interactions || interactions.length === 0) {
      console.log("[dipsy-training-evaluator] No interactions to evaluate.");
      return jsonResponse(200, {
        ok: true,
        processed: 0,
        failures: [],
        message: "No questions_brain interactions found to evaluate.",
      });
    }

    let processed = 0;
    const failures: { interaction_id: string; error: string }[] = [];

    for (const row of interactions as InteractionRow[]) {
      if (!row.question || !row.answer) {
        failures.push({
          interaction_id: row.id,
          error: "Missing question or answer.",
        });
        continue;
      }

      try {
        const evalJson = await runEvaluator(row.question, row.answer);

        if (
          evalJson == null ||
          typeof evalJson.overall_score !== "number" ||
          !evalJson.dimensions ||
          typeof evalJson.verdict !== "string"
        ) {
          throw new Error("Evaluator returned malformed JSON.");
        }

        // 2) Upsert into dipsy_training_evaluations
        const { error: upsertErr } = await supabase
          .from("dipsy_training_evaluations")
          .upsert(
            {
              interaction_id: row.id,
              org_id: row.org_id,
              user_id: row.user_id,
              agent_type: row.agent_type ?? "questions_brain",
              channel: row.channel,
              question: row.question,
              answer: row.answer,
              evaluation: evalJson,
              overall_score: evalJson.overall_score,
              verdict: evalJson.verdict,
            },
            { onConflict: "interaction_id" },
          );

        if (upsertErr) {
          console.error("[dipsy-training-evaluator] Upsert eval error:", upsertErr);
          failures.push({
            interaction_id: row.id,
            error: upsertErr.message || "Failed to upsert evaluation.",
          });
          continue;
        }

        // 3) ALSO sync into dipsy_training_examples so UI + rewriter can see it
        const { error: exampleUpdateErr } = await supabase
          .from("dipsy_training_examples")
          .update({
            evaluation: evalJson,
            overall_score: evalJson.overall_score,
            verdict: evalJson.verdict,
          })
          .eq("linked_interaction_id", row.id);

        if (exampleUpdateErr) {
          console.error(
            "[dipsy-training-evaluator] Failed to sync into dipsy_training_examples:",
            exampleUpdateErr,
          );
          // We don't treat this as a hard failure for the run, but we log it.
        }

        processed++;
      } catch (e) {
        console.error(
          "[dipsy-training-evaluator] Evaluation failed for interaction",
          row.id,
          e,
        );
        failures.push({
          interaction_id: row.id,
          error: e instanceof Error ? e.message : String(e),
        });
      }
    }

    console.log(
      `[dipsy-training-evaluator] Completed. processed=${processed}, failures=${failures.length}`,
    );

    return jsonResponse(200, {
      ok: true,
      processed,
      failures,
    });
  } catch (err) {
    console.error("[dipsy-training-evaluator] Unhandled error:", err);
    return jsonResponse(500, {
      ok: false,
      error: err instanceof Error ? err.message : String(err),
    });
  }
});

// --------------- Helpers ---------------

function jsonResponse(status: number, data: unknown): Response {
  return new Response(JSON.stringify(data), {
    status,
    headers: {
      ...corsHeaders,
      "Content-Type": "application/json",
    },
  });
}

async function runEvaluator(
  question: string,
  answer: string,
): Promise<EvaluationJson> {
  const payload = {
    model: "gpt-4.1-mini",
    messages: [
      { role: "system", content: EVALUATOR_SYSTEM_PROMPT },
      {
        role: "user",
        content: JSON.stringify({ question, answer }),
      },
    ],
  };

  const res = await fetch("https://api.openai.com/v1/chat/completions", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${OPENAI_API_KEY}`,
    },
    body: JSON.stringify(payload),
  });

  if (!res.ok) {
    const txt = await res.text();
    console.error("[runEvaluator] OpenAI error:", res.status, txt);
    throw new Error(`OpenAI evaluator error ${res.status}`);
  }

  const data = await res.json();
  const message = data.choices?.[0]?.message;

  if (!message || !message.content) {
    throw new Error("OpenAI evaluator returned no content.");
  }

  const contentStr =
    typeof message.content === "string" ? message.content : String(message.content);

  let parsed: EvaluationJson;
  try {
    parsed = JSON.parse(contentStr);
  } catch (e) {
    console.error("[runEvaluator] Failed to parse evaluator JSON:", e, contentStr);
    throw new Error("Failed to parse evaluator JSON.");
  }

  return parsed;
}
